# -*- coding: utf-8 -*-
"""TextClassificationTwitteranalysis.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1lBrgb-PNv2YLrrpceY9Tmy9elJJl97Ie
"""

#File: sentiment_mod.py

import nltk
import random
from nltk.corpus import movie_reviews
import pickle
from nltk.classify.scikitlearn import SklearnClassifier
from sklearn.naive_bayes import MultinomialNB,GaussianNB,BernoulliNB
from sklearn.linear_model import LogisticRegression,SGDClassifier
from sklearn.svm import SVC,LinearSVC,NuSVC
from nltk.classify import ClassifierI
from statistics import mode
from nltk.tokenize import word_tokenize

class VoteClassifier(ClassifierI):
  def __init__(self,*classifiers):
    self._classifiers=classifiers
  def classify(self,features):
    votes=[]
    for c in self._classifiers:
      v=c.classify(features)
      votes.append(v)
    return mode(votes)  
  def confidence(self,features):
    votes=[]
    for c in self._classifiers:
      v=c.classify(features)
      votes.append(v)
    choice_votes=votes.count(mode(votes))
    conf=choice_votes/len(votes)
    return conf

nltk.download("movie_reviews")

import zipfile

zipref=zipfile.ZipFile("../content/short_reviews.zip")
zipref.extractall()
zipref.close()

short_pos=open("../content/short_reviews/positive.txt","r",encoding = "ISO-8859-1").read()

short_neg=open("../content/short_reviews/negative.txt","r",encoding = "ISO-8859-1").read()

nltk.download('averaged_perceptron_tagger')

documents=[]
allowed_word_types=["J"]
all_words=[]
for p in short_pos.split('\n'):
  documents.append((p,"pos"))
  words=word_tokenize(p)
  pos=nltk.pos_tag(words)
  for w in pos:
    if w[1][0] in allowed_word_types:
      all_words.append(w[0].lower())
for p in short_neg.split('\n'):
  documents.append((p,"neg"))
  words=word_tokenize(p)
  pos=nltk.pos_tag(words)
  for w in pos:
    if w[1][0] in allowed_word_types:
      all_words.append(w[0].lower())



save_classifier1=open("documentsf.pickle","wb")
pickle.dump(documents,save_classifier1)
save_classifier1.close()

nltk.download('punkt')

##all_words=[]
##short_pos_words=word_tokenize(short_pos)
##short_neg_words=word_tokenize(short_neg)

##for w in short_pos_words:
##  all_words.append(w.lower())

##for w in short_neg_words:
##  all_words.append(w.lower())

all_words=nltk.FreqDist(all_words)
word_features=list(all_words.keys())[:5000]

save_feature=open("wordfeatures_f.pickle","wb")
pickle.dump(word_features,save_feature)
save_feature.close()

def find_features(document):
  words=word_tokenize(document)
  features={}
  for w in word_features:
    features[w]=( w in words)
  return features

featuresets=[(find_features(rev),category) for (rev,category) in documents]
random.shuffle(featuresets)



save_classifier1=open("featureset.pickle","wb")
pickle.dump(featuresets,save_classifier1)
save_classifier1.close()

training_set=featuresets[:10000]
testing_set=featuresets[10000:]

classifier=nltk.NaiveBayesClassifier.train(training_set)

print("NaiveBayes Algo accuracy:",(nltk.classify.accuracy(classifier,testing_set))*100)
classifier.show_most_informative_features(15)

save_classifier=open("naivebayes.pickle","wb")
pickle.dump(classifier,save_classifier)
save_classifier.close()

MNB_classifier=SklearnClassifier((MultinomialNB()))
MNB_classifier.train(training_set)
print("MNB_classifier accuracy",(nltk.classify.accuracy(MNB_classifier,testing_set))*100)

save_classifier1=open("MNB.pickle","wb")
pickle.dump(MNB_classifier,save_classifier1)
save_classifier1.close()

BNB_classifier=SklearnClassifier((BernoulliNB()))
BNB_classifier.train(training_set)
print("BNB_classifier accuracy",(nltk.classify.accuracy(BNB_classifier,testing_set))*100)



save_classifier1=open("BNB.pickle","wb")
pickle.dump(BNB_classifier,save_classifier1)
save_classifier1.close()

LogisticRegressionClassifier=SklearnClassifier((LogisticRegression()))
LogisticRegressionClassifier.train(training_set)
print("LR_classifier accuracy",(nltk.classify.accuracy(LogisticRegressionClassifier,testing_set))*100)



save_classifier1=open("LR.pickle","wb")
pickle.dump(LogisticRegressionClassifier,save_classifier1)
save_classifier1.close()

SGD_classifier=SklearnClassifier((SGDClassifier()))
SGD_classifier.train(training_set)
print("SGD_classifier accuracy",(nltk.classify.accuracy(SGD_classifier,testing_set))*100)



save_classifier1=open("SGD.pickle","wb")
pickle.dump(SGD_classifier,save_classifier1)
save_classifier1.close()

LinearSVC_classifier=SklearnClassifier((LinearSVC()))
LinearSVC_classifier.train(training_set)
print("LSVC_classifier accuracy",(nltk.classify.accuracy(LinearSVC_classifier,testing_set))*100)



save_classifier1=open("LSVC.pickle","wb")
pickle.dump(LinearSVC_classifier,save_classifier1)
save_classifier1.close()

NU_classifier=SklearnClassifier((NuSVC()))
NU_classifier.train(training_set)
print("NU_classifier accuracy",(nltk.classify.accuracy(NU_classifier,testing_set))*100)

save_classifier1=open("NU.pickle","wb")
pickle.dump(NU_classifier,save_classifier1)
save_classifier1.close()

voted_classifier=VoteClassifier(classifier,
                                MNB_classifier,
                                BNB_classifier,
                                SGD_classifier,
                                LinearSVC_classifier,
                                LogisticRegressionClassifier,
                                NU_classifier)

print("Votd classifier",(nltk.classify.accuracy(voted_classifier,testing_set))*100)

print("Classification:", voted_classifier.classify(testing_set[0][0]),"Confidence",voted_classifier.confidence(testing_set[0][0]))

def sentiment(text):
  feats=find_features(text)
  return voted_classifier.classify(feats)